{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic research\n",
    "============\n",
    "\n",
    "This notebook looks into the possibilities to give insights into a corpus through unsupervised topic detection. It looks at the basic methods of tfidf and word embeddings. It is not yet looking at LDA or other unsupervised clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import spacy\n",
    "\n",
    "from readers import JsonReader\n",
    "from analysis import TopicDetector, TopicDetector1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from spacy.lang.nl.stop_words import STOP_WORDS\n",
    "STOP_WORDS = \"english\"  # takes stop words from sklearn\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "json_reader = JsonReader(source=\"death_penalty.json\", subjects=[\"death\", \"penalty\"])\n",
    "texts = json_reader.get_texts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tfidf sums\n",
    "-------------\n",
    "\n",
    "This section looks at creating ngram word frames and creating sums of these to get to a topic list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_frame(texts, ngram, sums=True):\n",
    "    tfidf_vectorizer = TfidfVectorizer(stop_words=STOP_WORDS, ngram_range=(ngram, ngram))\n",
    "    tfidf_vectorizer.fit(texts)\n",
    "    feature_names = tfidf_vectorizer.get_feature_names()\n",
    "    tfidf_vectors = tfidf_vectorizer.transform(texts)\n",
    "    frame = pd.DataFrame(tfidf_vectors.toarray(), columns=feature_names)\n",
    "    if ngram == 1:\n",
    "        number_features = [feature for feature in feature_names if not feature.isalpha()]\n",
    "        frame.drop(labels=number_features, axis=1, inplace=True)\n",
    "    return frame.sum(axis=0) if sums else frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fako/miniconda3/envs/ml/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:1059: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    }
   ],
   "source": [
    "tfidf_frame = get_word_frame(texts, 1, sums=False)\n",
    "tfidf_words_sorted = tfidf_frame.sum(axis=0).sort_values(ascending=False)\n",
    "#tfidf_words_sorted_bi = get_word_frame(texts, 2)\n",
    "#tfidf_words_sorted_bi.sort_values(ascending=False, inplace=True)\n",
    "#tri_frame = get_word_frame(texts, 3)\n",
    "#tfidf_words_sorted_tri = tri_frame.sum(axis=0).sort_values(ascending=False)\n",
    "#tetra_frame = get_word_frame(texts, 4)\n",
    "#tfidf_words_sorted_tetra = tetra_frame.sum(axis=0).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "death         33.849226\n",
      "penalty       24.563764\n",
      "row           14.395764\n",
      "punishment     9.664681\n",
      "court          9.277523\n",
      "state          9.233713\n",
      "capital        9.089691\n",
      "said           7.893743\n",
      "execution      7.819045\n",
      "life           7.427824\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', 5):\n",
    "    print(tfidf_words_sorted[:10])\n",
    "\n",
    "#tfidf_words_sorted_bi\n",
    "#tfidf_words_sorted_tri\n",
    "#tfidf_words_sorted_tetra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process above of making frames per ngram has been automized through the TopicDetector. There are two version. TopicDetector1 is the original version crafted with this notebook. It only supports English. The current version (TopicDetector) is more memory efficient and supports Dutch. This detector works in production. Here we compare the output of the two version for quality assurance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fako/miniconda3/envs/ml/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:1059: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['death penalty information center', 1.9240768079324992],\n",
       " ['help improve online experience', 1.3726796729596455],\n",
       " ['site uses cookies help', 1.3726796729596455],\n",
       " ['uses cookies help improve', 1.3726796729596455],\n",
       " ['cookies help improve online', 1.3726796729596455],\n",
       " ['arabia seeks death penalty', 1.2234212999172254],\n",
       " ['saudi arabia seeks death', 1.2234212999172254],\n",
       " ['amnesty international site uses', 1.0379342145438972],\n",
       " ['international amnesty international site', 1.0379342145438972],\n",
       " ['amnesty international skip main', 1.0379342145438972],\n",
       " ['death row records', 0.7265589527499751],\n",
       " ['people death row', 0.4631655057691228],\n",
       " ['death penalty new', 0.39693434619882884],\n",
       " ['mumia abu jamal', 0.38542143825936626],\n",
       " ['sentenced life prison', 0.37157138317259486],\n",
       " ['death lethal injection', 0.3638097536717997],\n",
       " ['death penalty said', 0.363570873989616],\n",
       " ['victims family members', 0.33809783507194774],\n",
       " ['international human rights', 0.3314568658817309],\n",
       " ['death penalty unconstitutional', 0.3200876340933902],\n",
       " ['capital cases', 0.8348414725110236],\n",
       " ['electric chair', 0.8129074996612764],\n",
       " ['associated press', 0.6177863556180089],\n",
       " ['governor brown', 0.5553326931566842],\n",
       " ['death chamber', 0.5461670740269865],\n",
       " ['mr hinton', 0.5317746598944397],\n",
       " ['dr dre', 0.5276921752884912],\n",
       " ['prosecutorial misconduct', 0.5235698571388234],\n",
       " ['prison reform', 0.51778334730775],\n",
       " ['years later', 0.5090102733865559],\n",
       " ['wrote', 1.4364113887415775],\n",
       " ['left', 1.151838869790321],\n",
       " ['percent', 1.1113991780329162],\n",
       " ['reported', 1.0574725240971232],\n",
       " ['god', 0.9893636683114774],\n",
       " ['investigation', 0.952748813868815],\n",
       " ['result', 0.9457020702784154],\n",
       " ['able', 0.9078643591073647],\n",
       " ['month', 0.9077227173374801],\n",
       " ['got', 0.8783288047338974]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td = TopicDetector(lambda text: text)\n",
    "td.run(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fako/miniconda3/envs/ml/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:1059: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    }
   ],
   "source": [
    "td1 = TopicDetector1()\n",
    "td1(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('death penalty information center', 4, 1.9240768079324986),\n",
       " ('help improve online experience', 4, 1.3726796729596453),\n",
       " ('site uses cookies help', 4, 1.3726796729596453),\n",
       " ('uses cookies help improve', 4, 1.3726796729596453),\n",
       " ('cookies help improve online', 4, 1.3726796729596453),\n",
       " ('arabia seeks death penalty', 4, 1.2234212999172256),\n",
       " ('saudi arabia seeks death', 4, 1.2234212999172256),\n",
       " ('amnesty international site uses', 4, 1.0379342145438972),\n",
       " ('international amnesty international site', 4, 1.0379342145438972),\n",
       " ('amnesty international skip main', 4, 1.0379342145438972),\n",
       " ('death row inmate', 3, 3.3824444067473713),\n",
       " ('death row inmates', 3, 2.2086166482165432),\n",
       " ('abolish death penalty', 3, 1.7496502727860928),\n",
       " ('years death row', 3, 1.3803295831273121),\n",
       " ('new york times', 3, 1.2243457105756672),\n",
       " ('death penalty cases', 3, 1.216477621946909),\n",
       " ('support death penalty', 3, 1.1884812437281662),\n",
       " ('death row prisoners', 3, 1.0963549766828462),\n",
       " ('use death penalty', 3, 1.0381809117074952),\n",
       " ('abolition death penalty', 3, 1.0274190430715726),\n",
       " ('capital punishment', 2, 6.914500302728989),\n",
       " ('supreme court', 2, 4.636472257272567),\n",
       " ('human rights', 2, 3.528635021124888),\n",
       " ('sentenced death', 2, 2.8643579010624336),\n",
       " ('lethal injection', 2, 2.765356224302949),\n",
       " ('death sentence', 2, 2.407807100518795),\n",
       " ('death sentences', 2, 2.379900967356807),\n",
       " ('united states', 2, 2.0327640444847903),\n",
       " ('year old', 2, 2.004613232321926),\n",
       " ('eye eye', 2, 1.9268190263607468),\n",
       " ('said', 1, 7.893743446761297),\n",
       " ('execution', 1, 7.8190445244262845),\n",
       " ('people', 1, 6.513280850589858),\n",
       " ('crime', 1, 6.062739706650697),\n",
       " ('murder', 1, 5.799778337694477),\n",
       " ('executions', 1, 5.792807233127965),\n",
       " ('case', 1, 5.4728368132223935),\n",
       " ('texas', 1, 5.362234145136388),\n",
       " ('law', 1, 5.300102414097263),\n",
       " ('evidence', 1, 4.785976555303124)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "for ix, serie in td1.sorted_ngrams.items():\n",
    "    results += [(topic, len(topic.split(\" \")), importance) for topic, importance in serie[:10].items()]\n",
    "results.sort(key=lambda result: (result[1], result[2],), reverse=True)\n",
    "results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some differences between the TopicDetector and TopicDetector1. However these are due to one simple difference. The original TopicDetector1 takes tfidf values above 0.95, while TopicDetector takes the top 5% of terms. I think that the latter approach is more accurate and leads to more interesting results in the case of the death penalty corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word simularity\n",
    "------------------\n",
    "\n",
    "The cell below prints all words that are similar to each other according to the large English word vectors by spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = tfidf_words_sorted.index.tolist()\n",
    "all_tokens = list(map(lambda word: nlp.vocab[word], all_words))\n",
    "most_important_tokens = all_tokens[:50]\n",
    "for important_token in most_important_tokens:\n",
    "    similarities = []\n",
    "    for token in all_tokens:\n",
    "        if token is important_token:\n",
    "            continue\n",
    "        similarities.append((token.text, important_token.similarity(token),))\n",
    "    similarities = sorted(similarities, key=lambda item: item[1])\n",
    "    print(important_token.text)\n",
    "    print(\"*\" * 10 + \"most similar\" + \"*\" * 10)\n",
    "    most_similar = similarities[-5:]\n",
    "    most_similar.reverse()\n",
    "    print(\"\\n\".join(\n",
    "        \"{0} {1:.2f}\".format(word, similarity)\n",
    "        for word, similarity in most_similar\n",
    "    ))\n",
    "    print(\"*\" * 10 + \"most different\" + \"*\" * 10)\n",
    "    print(\"\\n\".join(\n",
    "        \"{0} {1:.2f}\".format(word, similarity)\n",
    "        for word, similarity in similarities[:5]\n",
    "    ))\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Co-occurance\n",
    "-----------------\n",
    "\n",
    "Below are some early experiments with co-occurance of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = tfidf_words_sorted.index.tolist()\n",
    "least_important_words = all_words[500:]\n",
    "most_important_frame = tfidf_frame.drop(labels=least_important_words, axis=1)\n",
    "\n",
    "most_important_cooccurence = most_important_frame.T.dot(most_important_frame)\n",
    "#np.fill_diagonal(most_important_cooccurence.values, 0)\n",
    "#most_important_cooccurence = most_important_cooccurence.applymap(lambda v: v if v >= 0.3 else 0.0)\n",
    "\n",
    "most_important_cooccurence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mic_sum = most_important_cooccurence.sum(axis=0).sort_values()\n",
    "mic_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "life_text_ixs = tfidf_frame[\"life\"].argsort()[::-1]\n",
    "justice_text_ixs = tfidf_frame[\"justice\"].argsort()[::-1]\n",
    "court_text_ixs = tfidf_frame[\"court\"].argsort()[::-1]\n",
    "law_text_ixs = tfidf_frame[\"law\"].argsort()[::-1]\n",
    "reward_text_ids = tfidf_frame[\"reward\"].argsort()[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "life_text_ixs = tfidf_frame[\"life\"].argsort()\n",
    "justice_text_ixs = tfidf_frame[\"justice\"].argsort()\n",
    "court_text_ixs = tfidf_frame[\"court\"].argsort()\n",
    "law_text_ixs = tfidf_frame[\"law\"].argsort()\n",
    "reward_text_ids = tfidf_frame[\"reward\"].argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts[life_text_ixs.iloc[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts[justice_text_ixs.iloc[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts[court_text_ixs.iloc[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts[law_text_ixs.iloc[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts[reward_text_ids.iloc[5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mins = most_important_cooccurence.min()\n",
    "nzeros = mins[mins > 0]\n",
    "frame = most_important_cooccurence.drop(labels=nzeros.index, axis=0)\n",
    "frame = frame.drop(labels=nzeros.index, axis=1)\n",
    "frame.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = [{\"name\": column, \"group\": 0} for column in frame.columns]\n",
    "node_names = [node[\"name\"] for node in nodes]\n",
    "links = [{\"source\": node_names.index(column), \"target\": node_names.index(key), \"value\": 1} for column, row in frame.iteritems() for key, value in row.iteritems() if value == 0]\n",
    "with open(\"../data/cooccurence-graph.json\", \"w\") as file:\n",
    "    json.dump({\n",
    "        \"nodes\": nodes,\n",
    "        \"links\": links\n",
    "    }, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml]",
   "language": "python",
   "name": "conda-env-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
