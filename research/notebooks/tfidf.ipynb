{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import spacy\n",
    "from spacy.lang.nl.stop_words import STOP_WORDS\n",
    "\n",
    "from readers import JsonReader\n",
    "from analysis import TopicDetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"nl_core_news_sm\")\n",
    "json_reader = JsonReader(source=\"biomassa.json\", subjects=[\"biomassa\"])\n",
    "texts = json_reader.get_texts()\n",
    "#from data.biomass import *\n",
    "#texts = [TEXT_1, TEXT_2, TEXT_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_frame(texts, ngram, sums=True):\n",
    "    tfidf_vectorizer = TfidfVectorizer(stop_words=STOP_WORDS, ngram_range=(ngram, ngram))\n",
    "    tfidf_vectorizer.fit(texts)\n",
    "    feature_names = tfidf_vectorizer.get_feature_names()\n",
    "    tfidf_vectors = tfidf_vectorizer.transform(texts)\n",
    "    frame = pd.DataFrame(tfidf_vectors.toarray(), columns=feature_names)\n",
    "    if ngram == 1:\n",
    "        number_features = [feature for feature in feature_names if not feature.isalpha()]\n",
    "        frame.drop(labels=number_features, axis=1, inplace=True)\n",
    "    return frame.sum(axis=0) if sums else frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf_frame = get_word_frame(texts, 1, sums=False)\n",
    "#tfidf_words_sorted = tfidf_frame.sum(axis=0).sort_values(ascending=False)\n",
    "\n",
    "tfidf_words_sorted_bi = get_word_frame(texts, 2)\n",
    "tfidf_words_sorted_bi.sort_values(ascending=False, inplace=True)\n",
    "#tri_frame = get_word_frame(texts, 3)\n",
    "#tfidf_words_sorted_tri = tri_frame.sum(axis=0).sort_values(ascending=False)\n",
    "#tetra_frame = get_word_frame(texts, 4)\n",
    "#tfidf_words_sorted_tetra = tetra_frame.sum(axis=0).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', 5):\n",
    "    print(tfidf_words_sorted_bi[:10])\n",
    "\n",
    "#tfidf_words_sorted_bi\n",
    "#tfidf_words_sorted_tri\n",
    "#tfidf_words_sorted_tetra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = tfidf_words_sorted.index.tolist()\n",
    "all_tokens = list(map(lambda word: nlp.vocab[word], all_words))\n",
    "most_important_tokens = all_tokens[:50]\n",
    "for important_token in most_important_tokens:\n",
    "    similarities = []\n",
    "    for token in all_tokens:\n",
    "        if token is important_token:\n",
    "            continue\n",
    "        similarities.append((token.text, important_token.similarity(token),))\n",
    "    similarities = sorted(similarities, key=lambda item: item[1])\n",
    "    print(important_token.text)\n",
    "    print(\"*\" * 10 + \"most similar\" + \"*\" * 10)\n",
    "    most_similar = similarities[-5:]\n",
    "    most_similar.reverse()\n",
    "    print(\"\\n\".join(\n",
    "        \"{0} {1:.2f}\".format(word, similarity)\n",
    "        for word, similarity in most_similar\n",
    "    ))\n",
    "    print(\"*\" * 10 + \"most different\" + \"*\" * 10)\n",
    "    print(\"\\n\".join(\n",
    "        \"{0} {1:.2f}\".format(word, similarity)\n",
    "        for word, similarity in similarities[:5]\n",
    "    ))\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = tfidf_words_sorted.index.tolist()\n",
    "least_important_words = all_words[500:]\n",
    "most_important_frame = tfidf_frame.drop(labels=least_important_words, axis=1)\n",
    "\n",
    "most_important_cooccurence = most_important_frame.T.dot(most_important_frame)\n",
    "#np.fill_diagonal(most_important_cooccurence.values, 0)\n",
    "#most_important_cooccurence = most_important_cooccurence.applymap(lambda v: v if v >= 0.3 else 0.0)\n",
    "\n",
    "most_important_cooccurence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mic_sum = most_important_cooccurence.sum(axis=0).sort_values()\n",
    "mic_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "life_text_ixs = tfidf_frame[\"life\"].argsort()[::-1]\n",
    "justice_text_ixs = tfidf_frame[\"justice\"].argsort()[::-1]\n",
    "court_text_ixs = tfidf_frame[\"court\"].argsort()[::-1]\n",
    "law_text_ixs = tfidf_frame[\"law\"].argsort()[::-1]\n",
    "reward_text_ids = tfidf_frame[\"reward\"].argsort()[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "life_text_ixs = tfidf_frame[\"life\"].argsort()\n",
    "justice_text_ixs = tfidf_frame[\"justice\"].argsort()\n",
    "court_text_ixs = tfidf_frame[\"court\"].argsort()\n",
    "law_text_ixs = tfidf_frame[\"law\"].argsort()\n",
    "reward_text_ids = tfidf_frame[\"reward\"].argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts[life_text_ixs.iloc[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts[justice_text_ixs.iloc[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts[court_text_ixs.iloc[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts[law_text_ixs.iloc[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts[reward_text_ids.iloc[5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mins = most_important_cooccurence.min()\n",
    "nzeros = mins[mins > 0]\n",
    "frame = most_important_cooccurence.drop(labels=nzeros.index, axis=0)\n",
    "frame = frame.drop(labels=nzeros.index, axis=1)\n",
    "frame.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = [{\"name\": column, \"group\": 0} for column in frame.columns]\n",
    "node_names = [node[\"name\"] for node in nodes]\n",
    "links = [{\"source\": node_names.index(column), \"target\": node_names.index(key), \"value\": 1} for column, row in frame.iteritems() for key, value in row.iteritems() if value == 0]\n",
    "with open(\"../data/cooccurence-graph.json\", \"w\") as file:\n",
    "    json.dump({\n",
    "        \"nodes\": nodes,\n",
    "        \"links\": links\n",
    "    }, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fako/miniconda3/envs/ml/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:1059: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['op het gebied van', 7.318341094985242],\n",
       " ['het gebruik van biomassa', 3.014911840658396],\n",
       " ['ministerie van economische zaken', 2.871670107429956],\n",
       " ['van het gebruik van', 2.547652273224091],\n",
       " ['van tak en tophout', 2.4615774061795843],\n",
       " ['met het oog op', 2.2387230921530272],\n",
       " ['wageningen ur food biobased', 2.119317704029361],\n",
       " ['ur food biobased research', 2.118398452288819],\n",
       " ['het ministerie van economische', 2.006552672604752],\n",
       " ['biobased research instituut binnen', 1.975358334433946],\n",
       " ['alterra rapport 1320', 0.45583501114996805],\n",
       " ['alterra rapport 1813', 0.45298737073806383],\n",
       " ['special mei 2013', 0.3981506866188229],\n",
       " ['ecn 07 015', 0.39739530174594595],\n",
       " ['bosbeheerplan ocmw herentals', 0.37825219694423384],\n",
       " ['nederlandse economie 2009', 0.3614416575450811],\n",
       " ['ecn 07 030', 0.32433651802553753],\n",
       " ['bodemlabel advies asr', 0.3212653523344494],\n",
       " ['2018transitie agendacirculaire economie', 0.3027969478216],\n",
       " ['ocf themarapport toerisme', 0.29477336562363193],\n",
       " ['figuur 16', 0.40799399055113444],\n",
       " ['tu onderzoek', 0.3851850745332675],\n",
       " ['biomass bioproducts', 0.3696557451036293],\n",
       " ['figuur 17', 0.3624081316925944],\n",
       " ['inleiding het', 0.35470524820969296],\n",
       " ['reststromen bundel', 0.3448745229674158],\n",
       " ['praktijkproef dwingelderveld', 0.3422767489293816],\n",
       " ['414066mezbrochure indd', 0.33909843817814167],\n",
       " ['gemeente dronten', 0.3336255047940705],\n",
       " ['door lagere', 0.3134425746787236],\n",
       " ['benefits', 0.4409577063444557],\n",
       " ['considered', 0.42489451883982515],\n",
       " ['presented', 0.37151060541907166],\n",
       " ['measures', 0.3462716570119246],\n",
       " ['wageningenworld', 0.3451220128229617],\n",
       " ['voedselbossen', 0.3198966250489728],\n",
       " ['biomassconsult', 0.3060995854531056],\n",
       " ['assumed', 0.30601121209906246],\n",
       " ['leaves', 0.2970754819807007],\n",
       " ['daarenboven', 0.28844082597295295]]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td = TopicDetector(lambda text: text)\n",
    "td.run(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#td.sorted_ngrams[2]\n",
    "\n",
    "#td.sorted_ngrams[3]\n",
    "#len(td.sorted_ngrams[4].where(lambda value: value >= 0.95))\n",
    "td.sorted_ngrams[1]\n",
    "#minimum = td.sorted_ngrams[4].min()\n",
    "#maximum = td.sorted_ngrams[4].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14806228"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(td.sorted_ngrams[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.6788447046272064e-05"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td.sorted_ngrams[4][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_index = set()\n",
    "for ix in range(self.max_ngram, 1, -1):\n",
    "    drop_index = td._get_drop_index(self.sorted_ngrams[ix].index, drop_index)\n",
    "    self.sorted_ngrams[ix - 1].drop(labels=drop_index, inplace=True, errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for ix, serie in td.sorted_ngrams.items():\n",
    "    results += [(topic, len(topic.split(\" \")), importance) for topic, importance in serie[:10].items()]\n",
    "results.sort(key=lambda result: (result[1], result[2],), reverse=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "            (topic, importance,)\n",
    "            for topic, word_count, importance in\n",
    "            results\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml]",
   "language": "python",
   "name": "conda-env-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
